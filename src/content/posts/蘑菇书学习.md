---
title: "蘑菇书学习"
pubDate: "2025-10-13"
modifiedDate: "2025-10-13"
---

## 第三章
### 3.3.1 蒙特卡罗策略评估
> 接下来，我们对蒙特卡洛方法进行总结。为了得到评估 V(s)，我们采取了如下的步骤。
[蒙特卡罗策略评估](https://datawhalechina.github.io/easy-rl/#/chapter3/chapter3?id=_311-%e6%9c%89%e6%a8%a1%e5%9e%8b:~:text=%E6%8E%A5%E4%B8%8B%E6%9D%A5%EF%BC%8C%E6%88%91%E4%BB%AC,%E5%A6%82%E4%B8%8B%E7%9A%84%E6%AD%A5%E9%AA%A4%E3%80%82)

注意此处在时间步t状态s被访问并更新都是在回合结束之后进行的，这也是为什么蒙特卡罗方法只能用在有终止的马尔可夫决策过程中，因为所有的回报G(t)必须在完整回合结束后才能进行计算，在计算出每一步的G(t)后再重新走一遍该回合的轨迹，此时轨迹上每一步的G(t)都是已知的，再按照书中的V(s)的评估方法的步骤来评估。

[假设现在有样本...](https://datawhalechina.github.io/easy-rl/#/chapter3/chapter3?id=_311-%e6%9c%89%e6%a8%a1%e5%9e%8b:~:text=%E5%81%87%E8%AE%BE%E7%8E%B0%E5%9C%A8%E6%9C%89,mean%EF%BC%89%E7%9A%84%E5%BD%A2%E5%BC%8F%EF%BC%9A)

这里把经验均值转换成增量均值，此处样本的含义是指对某个特定状态s，每次访问到这个状态s得到的回报G，这里t是可以跨回合的，含义是仅针对状态s，在整个更新过程中访问到s的次数。看下面的增量式蒙特卡罗的更新过程就可以理解这里符号的真正含义了。这里学习率可以手工设置成特定的值来控制。

[将其不停迭代，最后可以收敛。如图 3.12 所示，贝尔曼期望备份有两层加和，即内部加和和外部加和，计算两次期望，得到一个更新。](https://datawhalechina.github.io/easy-rl/#/chapter3/chapter3?id=_311-%e6%9c%89%e6%a8%a1%e5%9e%8b:~:text=%E5%B0%86%E5%85%B6%E4%B8%8D%E5%81%9C%E8%BF%AD%E4%BB%A3%EF%BC%8C%E6%9C%80%E5%90%8E%E5%8F%AF%E4%BB%A5%E6%94%B6%E6%95%9B%E3%80%82%E5%A6%82%E5%9B%BE%203.12%20%E6%89%80%E7%A4%BA%EF%BC%8C%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E5%A4%87%E4%BB%BD%E6%9C%89%E4%B8%A4%E5%B1%82%E5%8A%A0%E5%92%8C%EF%BC%8C%E5%8D%B3%E5%86%85%E9%83%A8%E5%8A%A0%E5%92%8C%E5%92%8C%E5%A4%96%E9%83%A8%E5%8A%A0%E5%92%8C%EF%BC%8C%E8%AE%A1%E7%AE%97%E4%B8%A4%E6%AC%A1%E6%9C%9F%E6%9C%9B%EF%BC%8C%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%E6%9B%B4%E6%96%B0%E3%80%82)
这里说内部和和外部和就是括号内和括号外，括号外是对可能执行的所有动作求期望，括号内则是对执行的每个动作可能得到的所有状态的值求期望。

## 第五章 PPO
此处通过重要性采样把同策略换成异策略是一个用估计来接近近似的方法，即我们认为 $\theta'$和$\theta$是类似的，为了保证二者比较近似，就会有后面讲到的近端策略惩罚和近端策略裁剪，二者都是保证两个分布相近的方法。此处估计的思想是对每次更新参数后分布都发生变化，要在新分布上重新采样导致采样浪费了大量时间的一种妥协，我们只要找到一个相近的分布，在相近的分布上采样，随后更新原始分布，尽管原始分布会变化，但仍然和相近的分布相近，因此就可以重复使用在相近分布上一次采样得到的数据，只需乘上一个重要性因子，这样在相近分布上采样一次就可以多次利用而不需要每次更新分布后都重新采样。

## 第六章 DQN
[我们观察 π 的值，发现里面混杂了一些不是 π 的经验，这有没有关系？](https://datawhalechina.github.io/easy-rl/#/chapter6/chapter6?id=_65-%e7%bb%8f%e9%aa%8c%e5%9b%9e%e6%94%be:~:text=%E6%88%91%E4%BB%AC%E8%A7%82%E5%AF%9F,%E6%9C%89%E6%B2%A1%E6%9C%89%E5%85%B3%E7%B3%BB%EF%BC%9F)
这里回答中说到的过去采样使用的策略和现在的策略不是很像也没有关系，只采样了一笔经验的意思是，如果使用时序差分的方法来更新Q的话，此处Q的更新公式为
Q(s, a) ← Q(s, a) + α * \[r + γ * max_a' Q(s', a') - Q(s, a)\]
后面方括号内的项为时序差分目标，注意这里r是环境给的奖励，是在采样过程中得到的，而max_a' Q(s', a')这部分则是通过网络估计得出的，也就是说，在一次“经验”中，我们已经知道了(s, a, r, s')这样一个四元组：
- 在状态 `s`
- 执行了动作 `a`
- 得到了奖励 `r`
- 转移到了新状态 `s'`
那么此处max_a' Q(s', a')只依赖于当前的Q网络根据s'进行评估，也就是说记录和评估是异步发生的，在采样的时候采集的经验仅仅起到记录的作用，有效的是奖励r，而评估则是训练过程中用当前正在训练的网络进行评估的，因此什么时候评估都不影响，这就是时序差分中使用网络来估计行为价值的好处，即把评估和记录解耦开。

## 第七章 DQN进阶
[噪声网络](https://datawhalechina.github.io/easy-rl/#/chapter7/chapter7?id=_75-%e5%99%aa%e5%a3%b0%e7%bd%91%e7%bb%9c)
2017年的一篇文章讲了带噪声的稀疏门控混合专家网络，这里2018两篇文章都是针对深度Q网络加噪，很多不同领域的深度学习方法的思路都具有相通和通用性，关键还是理解背后的原因以及这些做法的优劣。

## 第八章 连续动作的DQN

